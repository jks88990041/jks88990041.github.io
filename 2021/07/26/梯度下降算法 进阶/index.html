<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>梯度下降算法 进阶 | 一只柴犬</title><meta name="description" content="@[TOC](梯度算法 进阶) 回顾梯度算法是一种迭代的算法，每看一个参数都会更新。) 学习率η（Learning Rate）自定义的学习率对参数选择的影响学习率需要选取合适的值，过小会导致模型调参的速度太慢，但过大会导致错失掉了最佳参数点 如何调整学习率η？ 在最开始的时候，随机点离目标点很远，我们一般会选取一个比较大的学习率；当做了几期后，我们离目标点很近了，所以我们会减小学习率 缩减为 如下"><meta name="author" content="凯凯超人"><meta name="copyright" content="凯凯超人"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://yoursite.com/2021/07/26/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%20%E8%BF%9B%E9%98%B6/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="梯度下降算法 进阶"><meta property="og:url" content="http://yoursite.com/2021/07/26/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%20%E8%BF%9B%E9%98%B6/"><meta property="og:site_name" content="一只柴犬"><meta property="og:description" content="@[TOC](梯度算法 进阶) 回顾梯度算法是一种迭代的算法，每看一个参数都会更新。) 学习率η（Learning Rate）自定义的学习率对参数选择的影响学习率需要选取合适的值，过小会导致模型调参的速度太慢，但过大会导致错失掉了最佳参数点 如何调整学习率η？ 在最开始的时候，随机点离目标点很远，我们一般会选取一个比较大的学习率；当做了几期后，我们离目标点很近了，所以我们会减小学习率 缩减为 如下"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/blogimg/picbed@master/2020/04/13/163a2ade4361d1ed705ed523091af67e.png"><meta property="article:published_time" content="2021-07-26T15:57:03.481Z"><meta property="article:modified_time" content="2021-07-26T15:57:10.677Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="next" title="模型误差的来源分析" href="http://yoursite.com/2021/07/25/%E6%A8%A1%E5%9E%8B%E8%AF%AF%E5%B7%AE%E7%9A%84%E6%9D%A5%E6%BA%90%E5%88%86%E6%9E%90/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true
  }</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/pool.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/iconfont.min.css"><link rel="stylesheet" href="/css/background.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/css/hideCategory.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/macWhite.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/font-awesome-animation.min.css"><meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="一只柴犬" type="application/atom+xml">
</head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/admin.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">36</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw https://dh.xiaokang.me/"></i><span> 搜索</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw faa-pulse animated"></i><span> 一只柴犬</span></a></div><div class="menus_item"><a class="site-page"><span> 关注我</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw faa-shake animated-hover"></i><span> 留言板</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw faa-pulse"></i><span> 关于我</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-link"></i><span> 友情链接</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw faa-rising animated-hover"></i><span> 友情链接</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw faa-falling animated-hover"></i><span> 网址收藏</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-tags"></i><span> 归档</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-tags"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 放轻松</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-film"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 小功能</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-lemon"></i><span> Iconfont库</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-snowflake"></i><span> 表情速查</span></a></li></ul></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#回顾梯度算法"><span class="toc-number">1.</span> <span class="toc-text">回顾梯度算法</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#学习率η（Learning-Rate）"><span class="toc-number">2.</span> <span class="toc-text">学习率η（Learning Rate）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#自定义的学习率对参数选择的影响"><span class="toc-number">2.1.</span> <span class="toc-text">自定义的学习率对参数选择的影响</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#如何调整学习率η？"><span class="toc-number">2.2.</span> <span class="toc-text">如何调整学习率η？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Adagrad"><span class="toc-number">2.2.1.</span> <span class="toc-text">Adagrad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stochastic-Gradient-Descent（SGD随机梯度下降法）-？？？"><span class="toc-number">2.2.2.</span> <span class="toc-text">Stochastic Gradient Descent（SGD随机梯度下降法） ？？？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#小批量随机梯度下降（batch-gradient-descent）"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">小批量随机梯度下降（batch gradient descent）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Feature-Scaling-（特征缩放）"><span class="toc-number">2.2.3.</span> <span class="toc-text">Feature Scaling （特征缩放）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#为什么要做Feature-Scaling？"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">为什么要做Feature Scaling？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#如何实现Feature-Scaling？"><span class="toc-number">2.2.3.2.</span> <span class="toc-text">如何实现Feature Scaling？</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#梯度下降算法数学总结"><span class="toc-number">3.</span> <span class="toc-text">梯度下降算法数学总结</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#首先要回顾泰勒公式"><span class="toc-number">3.1.</span> <span class="toc-text">首先要回顾泰勒公式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#用泰勒展开损失函数"><span class="toc-number">3.2.</span> <span class="toc-text">用泰勒展开损失函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#梯度下降算法缺点在哪"><span class="toc-number">4.</span> <span class="toc-text">梯度下降算法缺点在哪</span></a></li></ol></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(https://cdn.jsdelivr.net/gh/blogimg/picbed@master/2020/04/13/163a2ade4361d1ed705ed523091af67e.png)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">一只柴犬</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw https://dh.xiaokang.me/"></i><span> 搜索</span></a></div><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw faa-pulse animated"></i><span> 一只柴犬</span></a></div><div class="menus_item"><a class="site-page"><span> 关注我</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw faa-shake animated-hover"></i><span> 留言板</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw faa-pulse"></i><span> 关于我</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-link"></i><span> 友情链接</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw faa-rising animated-hover"></i><span> 友情链接</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw faa-falling animated-hover"></i><span> 网址收藏</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-book"></i><span> 找文章</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-tags"></i><span> 归档</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-tags"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 放轻松</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-film"></i><span> 影视</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 小功能</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-lemon"></i><span> Iconfont库</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fas fa-snowflake"></i><span> 表情速查</span></a></li></ul></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">梯度下降算法 进阶</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="Created 2021-07-26 23:57:03"><i class="far fa-calendar-alt fa-fw"></i> Created 2021-07-26</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="Updated 2021-07-26 23:57:10"><i class="fas fa-history fa-fw"></i> Updated 2021-07-26</span></time></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-commentcount"></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p>@[TOC](梯度算法 进阶)</p>
<h1 id="回顾梯度算法"><a href="#回顾梯度算法" class="headerlink" title="回顾梯度算法"></a>回顾梯度算法</h1><p><strong>是一种迭代的算法，每看一个参数都会更新。</strong><br><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/eee493cfe8f24755a2aa3cfd41f486f4.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">)<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/62ccf5ddd84348308c79a70233b0da19.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h1 id="学习率η（Learning-Rate）"><a href="#学习率η（Learning-Rate）" class="headerlink" title="学习率η（Learning Rate）"></a>学习率η（Learning Rate）</h1><h2 id="自定义的学习率对参数选择的影响"><a href="#自定义的学习率对参数选择的影响" class="headerlink" title="自定义的学习率对参数选择的影响"></a>自定义的学习率对参数选择的影响</h2><p><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/6c39ce49d85048098ac7303a552bc184.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>学习率需要选取合适的值，过小会导致模型调参的速度太慢，但过大会导致错失掉了最佳参数点</strong></p>
<h2 id="如何调整学习率η？"><a href="#如何调整学习率η？" class="headerlink" title="如何调整学习率η？"></a>如何调整学习率η？</h2><ol>
<li><p>在最开始的时候，随机点离目标点很远，我们一般会选取一个比较大的学习率；当做了几期后，我们离目标点很近了，所以我们会减小学习率 缩减为 如下图所示的公式<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/b9bc45580c004a6cbb708595125b1a1a.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
</li>
<li><p><strong>不同的参数应该设置不同的学习率</strong></p>
</li>
</ol>
<p>下面是常用的 调整学习率的方法：</p>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p>Adagrad是解决不同参数应该使用不同的更新速率的问题。<strong>Adagrad自适应地为各个参数分配不同学习率的算法。</strong></p>
<blockquote>
<p><strong>其原理为：</strong><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/4423c0afa60f462cb444f37cd1307576.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>里面比较核心的部分在 每次σ的取值。<strong>每次σ规定为之前所有<em>g</em>平方对应的之和的均方根</strong>。例如下图所示：其中 <em>g</em> 是每次的偏导值<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/82780afd2b0347ad8505db41024b9f7d.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>结合之前说的 <em>η</em> 值的变化 可以得到如下的公式推导：<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/cf79f34d94b44c72abdd283ebb0f3431.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
</blockquote>
<p><strong>提问:</strong>    发现一个现象，本来应该是随着gradient的增大，我们的学习率是希望增大的，也就是图中的g上标t；但是与此同时随着gradient的增大，我们的分母是在逐渐增大，也就对整体学习率是减少的，这是为什么呢？<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/64154f52e91f4486be92171800f09815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>这是因为随着我们更新次数的增大，我们是希望我们的学习率越来越慢。因为我们认为在学习率的最初阶段，我们是距离损失函数最优解很远的，随着更新的次数的增多，<strong>我们认为越来越接近最优解，于是学习速率也随之变慢。</strong>  为什么化简之后是如上这个式子呢，其在图像上的意义是 一阶导数比上二阶数的值。如下图所示：<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/90fda3afedce4958a877451546c006e2.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h3 id="Stochastic-Gradient-Descent（SGD随机梯度下降法）-？？？"><a href="#Stochastic-Gradient-Descent（SGD随机梯度下降法）-？？？" class="headerlink" title="Stochastic Gradient Descent（SGD随机梯度下降法） ？？？"></a>Stochastic Gradient Descent（SGD随机梯度下降法） ？？？</h3><p>其和 普通的梯度下降算法区别在。普通遍历在求和的时候需要浪费大量时间，进而去掉求和产生了随机梯度下降算法。<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/e92e87942f4443cbac50e52369c68151.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>和下图看到的一样：<br><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/76673f0f26614df6a9ac37311fe015b1.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<blockquote>
<p> 只是要注意一下标准的梯度下降和随机梯度下降的区别：</p>
<ol>
<li>标准下降时在权值更新前汇总所有样例得到的标准梯度，随机下降则是通过考察每次训练实例来更新（<strong>就是随机选择一些按顺序的后续样本点来，而不是全体数据</strong>）。<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/32da14cad9bf4a2ba439745929b7ec11.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></li>
</ol>
</blockquote>
<blockquote>
<ol start="2">
<li>对于步长 <em>η</em> 的取值，标准梯度下降的 <em>η</em> 比随机梯度下降的大。因为标准梯度下降的是使用准确的梯度，理直气壮地走，随机梯度下降<strong>使用的是近似的梯度</strong>，就得小心翼翼地走，怕一不小心误入歧途南辕北辙了。</li>
<li>当损失函数有多个局部极小值时，随机梯度反而更可能避免进入局部极小值中。</li>
</ol>
</blockquote>
<h4 id="小批量随机梯度下降（batch-gradient-descent）"><a href="#小批量随机梯度下降（batch-gradient-descent）" class="headerlink" title="小批量随机梯度下降（batch gradient descent）"></a>小批量随机梯度下降（batch gradient descent）</h4><p>如果在每次迭代中，梯度下降是用整个训练数据集来计算梯度的话，则会带来大量的计算量。因此提出批量梯度下降来进行优化。<strong>每次优化不再是对整体数据集来计算损失，取而代之使用随机采样小批量的样本来计算梯度。</strong><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/0d8e8d1e6c234e36b0972e5b95c9f46a.png#pic_center" alt="在这里插入图片描述"></p>
<h3 id="Feature-Scaling-（特征缩放）"><a href="#Feature-Scaling-（特征缩放）" class="headerlink" title="Feature Scaling （特征缩放）"></a>Feature Scaling （特征缩放）</h3><p>其意思就是说要<strong>将所有特征有相同的规模</strong>。例如下图所示，<em>X2</em> 的范围明显大宇 <em>X1*，所以要将 *X2</em> 进行缩放。<br><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/2b4681ef27884c0bbf400db0624e8740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h4 id="为什么要做Feature-Scaling？"><a href="#为什么要做Feature-Scaling？" class="headerlink" title="为什么要做Feature Scaling？"></a>为什么要做Feature Scaling？</h4><blockquote>
<p>为什么要做Feature Scaling？<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/456a5666c6bf40b7a1e5f7a8727b529f.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>如左图，<em>X1</em> 和 <em>X2</em> 数值规模大小相差很大，那 <em>W2</em> 这参数的变动会极大的影响损失函数，而 <em>W1</em> 影响度就很小。所以我们<strong>需要对 <em>X2</em> 进行特征缩放，使其与 <em>X1</em> 保持一个规模</strong>。并且其实从图像可以看出，如果按左图来做的话，我们很难找到一组合适的参数集合，因为他梯度下降的方法不是直线的而是曲线；而右图是近乎直线。（<strong>最里面圈的损失函数最小</strong>）</p>
</blockquote>
<h4 id="如何实现Feature-Scaling？"><a href="#如何实现Feature-Scaling？" class="headerlink" title="如何实现Feature Scaling？"></a>如何实现Feature Scaling？</h4><p><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/f129b6a000c841ec926df684d2c77a15.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<p>如上图所示，假如有R个数据样本，每个样本有<em>X1 - Xi</em> 个特征。我们用上面的公式计算出其应该的scale（其实说实在的这不就是化成标准正太分布么）</p>
<h1 id="梯度下降算法数学总结"><a href="#梯度下降算法数学总结" class="headerlink" title="梯度下降算法数学总结"></a>梯度下降算法数学总结</h1><p>提出问题：如下图所示<strong>（我怎么样才能在红圈里找到最小损失的那个点呢）</strong><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/53c25201a11349b6bc3af9e1dfc9fcc6.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="首先要回顾泰勒公式"><a href="#首先要回顾泰勒公式" class="headerlink" title="首先要回顾泰勒公式"></a>首先要回顾泰勒公式</h2><p><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/c9b154d4d14c43c8bf5e2609789bb094.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述">)<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/205bb83b97d849fc9b8c7451e044fdab.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>二元泰勒：<br><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/2efa8bf67ffa419e90221154515a3712.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h2 id="用泰勒展开损失函数"><a href="#用泰勒展开损失函数" class="headerlink" title="用泰勒展开损失函数"></a>用泰勒展开损失函数</h2><p><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/3ce8a2749e644d1f978c43d485798dde.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>理解为点乘，反向180度的时候，损失函数才是最小的：<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/235fc97c36b74012b0663975ba0dc8a9.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>最终可以表达成：<img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/83f0037797f64c048376868ee2c21fae.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
<h1 id="梯度下降算法缺点在哪"><a href="#梯度下降算法缺点在哪" class="headerlink" title="梯度下降算法缺点在哪"></a>梯度下降算法缺点在哪</h1><p>其不仅仅包括可能有找到是局部最优解问题，有可能在中间的时候就有偏微分为0的时候，而这时这个点可能离全局最优解点 很远。<br><img src= "/img/loading.gif" data-src="https://img-blog.csdnimg.cn/cd31675e6d8e4c83925852218d4f2acc.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2prczg4OTk1NjU2,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">凯凯超人</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yoursite.com/2021/07/26/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%20%E8%BF%9B%E9%98%B6/">http://yoursite.com/2021/07/26/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%20%E8%BF%9B%E9%98%B6/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/blogimg/picbed@master/2020/04/13/163a2ade4361d1ed705ed523091af67e.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><button class="reward-button"><i class="fas fa-qrcode"></i> Donate<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="post-qr-code__img" src="/img/wechat.jpg" alt="wechat" onclick="window.open('/img/wechat.jpg')"/><div class="post-qr-code__desc">wechat</div></li><li class="reward-item"><img class="post-qr-code__img" src="/img/alipay.jpg" alt="alipay" onclick="window.open('/img/alipay.jpg')"/><div class="post-qr-code__desc">alipay</div></li></ul></div></button></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2021/07/25/%E6%A8%A1%E5%9E%8B%E8%AF%AF%E5%B7%AE%E7%9A%84%E6%9D%A5%E6%BA%90%E5%88%86%E6%9E%90/"><img class="next-cover" data-src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/media/image/14.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">模型误差的来源分析</div></div></a></div></nav></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By 凯凯超人</div><div class="framework-info"><span>Driven </span><a href="https://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="icp"><a href="http://www.beian.gov.cn" target="_blank"><img class="lozad" data-src="/img/icp.png" onerror="onerror=null;src='/img/icp.png'" style="padding:0px;vertical-align: text-bottom;"/><span>浙ICP备12345678号</span></a></div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><button id="readmode" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font_plus" title="Increase Font Size"><i class="fas fa-plus"></i></button><button id="font_minus" title="Decrease Font Size"><i class="fas fa-minus"></i></button><button class="translate_chn_to_cht" id="translateLink" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/js/pool.js"></script><script src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"></script><script src="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/hideCategory.min.js"></script></body></html>